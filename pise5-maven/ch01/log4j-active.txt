23/02/08 00:14:59 WARN DecryptMergeMemreadSpin$: memreadDataFiltered3 #: 74696128
23/02/08 00:15:00 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
23/02/08 00:21:00 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:06 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:09 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:15 WARN DecryptMergeMemreadSpin$: saving row flags (# = 64, table: [abfss://rvdc@prodadls2u314.dfs.core.windows.net//adas_fault_tracing/task/ASDR.rowflag-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf//table], samples:
	[ad4f829634454aaca3b48d6175be7d02,WrappedArray(-5),todo]
	[50eb9a1c6e0f4d5aa546abf598c3497f,WrappedArray(-5),todo]
	[7e53ff95136945c38da990e2073227aa,WrappedArray(-5),todo]
	[126c7a427720406ca0afae1177999165,WrappedArray(-5),todo]
	[61f35689ddc74fba8a4dd9a93261eb8d,WrappedArray(-5),todo]
23/02/08 00:21:15 WARN DeltaTableHelpers$: Merging abfss://rvdc@prodadls2u314.dfs.core.windows.net//adas_fault_tracing/task/ASDR.rowflag-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf//table
23/02/08 00:21:24 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:36 WARN DeltaParquetFileFormat: Forcing use of DatabricksVectorizedParquetRecordReader and disabling native reader because special columns are requested in schema: StructField(spin,StringType,true)StructField(_databricks_internal_edge_computed_column_row_index,LongType,true)
23/02/08 00:21:36 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:40 WARN DeltaParquetFileFormat: Forcing use of DatabricksVectorizedParquetRecordReader and disabling native reader because special columns are requested in schema: StructField(spin,StringType,true)StructField(_databricks_internal_edge_computed_column_skip_row,BooleanType,true)
23/02/08 00:21:40 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:44 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:21:49 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
23/02/08 00:21:49 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
23/02/08 00:27:20 WARN DecryptMergeMemreadSpin$: memreadData #:
	(ff2d0b31d8b749edbf1b3c4213ce6642-1421-37590642,(1178,(36c5c3eb-c09d-43bf-a42b-d15189ffac46: [*960] [0] '[OK]',false),2022-12-02 09:34:33.451))
	(282fb24674a54c5196d0936d27fb13bc-1401-65852964,(256,(adf8511f-38e4-4167-ad1a-2c373c4d6c4a: [*8158] [0] '[OK]',false),2022-06-21 07:18:30.308))
	(066133c412924dcaa69617de7f8a6dad-1421-168219920,(755,(42508820-f0ca-4849-97c0-e5bda4bc5d5d: [*960] [0] '[OK]',false),2022-12-06 17:50:55.214))
	(1b4bceb34dfe45dcb0a82bc987de14e7-1401-107786543,(113,(ca7f5e88-35ae-4abc-973c-ef5c17fcd7c9: [*8158] [0] '[OK]',false),2023-01-10 18:26:34.939))
	(15a0280852ee45009d949c6898f79d1f-1401-29477505,(256,(1034f4a5-b8e6-4afd-bfad-192465f0e6b5: [*8158] [0] '[OK]',false),2022-04-20 13:43:59.925))
23/02/08 00:29:36 WARN DecryptMergeMemreadSpin$: mergedData #: 52807
23/02/08 00:29:37 ERROR AbfsClient: HttpRequest: 404,err=PathNotFound,appendpos=,cid=cb19a489-e7ac-49ad-baa9-3196d581bc66,rid=9dc60ee5-301f-000e-5f54-3b419a000000,connMs=0,sendMs=0,recvMs=16,sent=0,recv=163,method=GET,url=https://prodadls2u314.dfs.core.windows.net/rvdc?upn=false&resource=filesystem&maxResults=5000&directory=adas_fault_tracing/task/ASDR.merged-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf/table&timeout=90&recursive=false
23/02/08 00:29:37 WARN DeltaTableHelpers$: Creating abfss://rvdc@prodadls2u314.dfs.core.windows.net//adas_fault_tracing/task/ASDR.merged-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf//table
23/02/08 00:29:37 ERROR AbfsClient: HttpRequest: 404,err=PathNotFound,appendpos=,cid=eb172140-02b8-4802-adcb-033c40f308a8,rid=9dc60eed-301f-000e-6754-3b419a000000,connMs=0,sendMs=0,recvMs=11,sent=0,recv=163,method=GET,url=https://prodadls2u314.dfs.core.windows.net/rvdc?upn=false&resource=filesystem&maxResults=5000&directory=adas_fault_tracing/task/ASDR.merged-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf/table/_delta_log&continuation=NTU4NjA5OTE0NzQwMTQ1MDUyNyAwIDAwMDAwMDAwMDAwMDAwMDAwMDA%3D&timeout=90&recursive=false
23/02/08 00:29:37 ERROR AzureBlobFileSystemStore: Received exception while listing a directory.
Operation failed: "The specified path does not exist.", 404, GET, https://prodadls2u314.dfs.core.windows.net/rvdc?upn=false&resource=filesystem&maxResults=5000&directory=adas_fault_tracing/task/ASDR.merged-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf/table/_delta_log&continuation=NTU4NjA5OTE0NzQwMTQ1MDUyNyAwIDAwMDAwMDAwMDAwMDAwMDAwMDA%3D&timeout=90&recursive=false, PathNotFound, "The specified path does not exist. RequestId:9dc60eed-301f-000e-6754-3b419a000000 Time:2023-02-08T00:29:37.7185590Z"
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:256)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:347)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listFiles(AzureBlobFileSystemStore.java:1247)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.access$200(AzureBlobFileSystemStore.java:148)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore$2.fetchMoreResults(AzureBlobFileSystemStore.java:1339)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore$2.<init>(AzureBlobFileSystemStore.java:1329)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatusAsIterator(AzureBlobFileSystemStore.java:1325)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatusAsIterator(AzureBlobFileSystem.java:508)
	at com.databricks.tahoe.store.EnhancedAzureBlobFileSystemUpgrade.listStatus(EnhancedFileSystem.scala:592)
	at com.databricks.tahoe.store.EnhancedFileSystem.listStatusUnsafe(EnhancedFileSystem.scala:60)
	at com.databricks.tahoe.store.AzureLogStore.listFromUnsafe(AzureLogStore.scala:44)
	at com.databricks.tahoe.store.DelegatingLogStore.$anonfun$listFromUnsafe$1(DelegatingLogStore.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.tahoe.store.DelegatingLogStore.listFromUnsafe(DelegatingLogStore.scala:276)
	at com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.listFrom(SnapshotManagementEdge.scala:42)
	at com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.listFrom$(SnapshotManagementEdge.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaLog.listFrom(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$listDeltaAndCheckpointFiles$1(SnapshotManagement.scala:98)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withOperationTypeTag(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:139)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordOperation(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordDeltaOperation(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.listDeltaAndCheckpointFiles(SnapshotManagement.scala:94)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$getLogSegmentForVersion$1(SnapshotManagement.scala:140)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getLogSegmentForVersion(SnapshotManagement.scala:137)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getLogSegmentForVersion$(SnapshotManagement.scala:134)
	at com.databricks.sql.transaction.tahoe.DeltaLog.getLogSegmentForVersion(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getLogSegmentFrom(SnapshotManagement.scala:64)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getLogSegmentFrom$(SnapshotManagement.scala:62)
	at com.databricks.sql.transaction.tahoe.DeltaLog.getLogSegmentFrom(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:249)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:258)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:247)
	at com.databricks.sql.transaction.tahoe.DeltaLog.getSnapshotAtInit(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$init$(SnapshotManagement.scala:54)
	at com.databricks.sql.transaction.tahoe.DeltaLog.<init>(DeltaLog.scala:80)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:633)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:633)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.withOperationTypeTag(DeltaLog.scala:476)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.recordFrameProfile(DeltaLog.scala:476)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:139)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.recordOperation(DeltaLog.scala:476)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.recordDeltaOperation(DeltaLog.scala:476)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.createDeltaLog$1(DeltaLog.scala:632)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$apply$6(DeltaLog.scala:652)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:652)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:659)
	at com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:512)
	at com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:158)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:590)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:168)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:590)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:566)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:427)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)
	at com.volvocars.rvdc.common.DeltaTableHelpers$.saveOrUpdate(DeltaTableHelpers.scala:61)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.decrytpAndMerge(DecryptMergeMemreadBase.scala:318)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.$anonfun$run$1(DecryptMergeMemreadBase.scala:80)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.$anonfun$run$1$adapted(DecryptMergeMemreadBase.scala:65)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.run(DecryptMergeMemreadBase.scala:65)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.processMemRead(DecryptMergeMemreadBase.scala:59)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.run(DecryptMergeMemreadBase.scala:38)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadSpin$.main(DecryptMergeMemreadSpin.scala:7)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:43)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw$$iw.<init>(command--1:45)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw.<init>(command--1:47)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw.<init>(command--1:49)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw.<init>(command--1:51)
	at $line3860880a722c453fae7be822f691a0b525.$read.<init>(command--1:53)
	at $line3860880a722c453fae7be822f691a0b525.$read$.<init>(command--1:57)
	at $line3860880a722c453fae7be822f691a0b525.$read$.<clinit>(command--1)
	at $line3860880a722c453fae7be822f691a0b525.$eval$.$print$lzycompute(<notebook>:7)
	at $line3860880a722c453fae7be822f691a0b525.$eval$.$print(<notebook>:6)
	at $line3860880a722c453fae7be822f691a0b525.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1003)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:956)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:634)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:611)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)
	at java.lang.Thread.run(Thread.java:750)
23/02/08 00:29:37 ERROR AbfsClient: HttpRequest: 404,err=PathNotFound,appendpos=,cid=68f6e4c4-79c6-468b-b995-260d4e13560b,rid=9dc60eee-301f-000e-6854-3b419a000000,connMs=0,sendMs=0,recvMs=9,sent=0,recv=163,method=GET,url=https://prodadls2u314.dfs.core.windows.net/rvdc?upn=false&resource=filesystem&maxResults=5000&directory=adas_fault_tracing/task/ASDR.merged-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf/table/_delta_log&continuation=NTU4NjA5OTE0NzQwMTQ1MDUyNyAwIDAwMDAwMDAwMDAwMDAwMDAwMDA%3D&timeout=90&recursive=false
23/02/08 00:29:37 ERROR AzureBlobFileSystemStore: Received exception while listing a directory.
Operation failed: "The specified path does not exist.", 404, GET, https://prodadls2u314.dfs.core.windows.net/rvdc?upn=false&resource=filesystem&maxResults=5000&directory=adas_fault_tracing/task/ASDR.merged-memreadspin.700c4661-c1a3-40cd-86c5-2b3e52f67aaf/table/_delta_log&continuation=NTU4NjA5OTE0NzQwMTQ1MDUyNyAwIDAwMDAwMDAwMDAwMDAwMDAwMDA%3D&timeout=90&recursive=false, PathNotFound, "The specified path does not exist. RequestId:9dc60eee-301f-000e-6854-3b419a000000 Time:2023-02-08T00:29:37.7623665Z"
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:256)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:347)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listFiles(AzureBlobFileSystemStore.java:1247)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.access$200(AzureBlobFileSystemStore.java:148)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore$2.fetchMoreResults(AzureBlobFileSystemStore.java:1339)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore$2.<init>(AzureBlobFileSystemStore.java:1329)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatusAsIterator(AzureBlobFileSystemStore.java:1325)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatusAsIterator(AzureBlobFileSystem.java:508)
	at com.databricks.tahoe.store.EnhancedAzureBlobFileSystemUpgrade.listStatus(EnhancedFileSystem.scala:592)
	at com.databricks.tahoe.store.EnhancedFileSystem.listStatusUnsafe(EnhancedFileSystem.scala:60)
	at com.databricks.tahoe.store.AzureLogStore.listFromUnsafe(AzureLogStore.scala:44)
	at com.databricks.tahoe.store.DelegatingLogStore.$anonfun$listFromUnsafe$1(DelegatingLogStore.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.tahoe.store.DelegatingLogStore.listFromUnsafe(DelegatingLogStore.scala:276)
	at com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.listFrom(SnapshotManagementEdge.scala:42)
	at com.databricks.sql.transaction.tahoe.SnapshotManagementEdge.listFrom$(SnapshotManagementEdge.scala:39)
	at com.databricks.sql.transaction.tahoe.DeltaLog.listFrom(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$listDeltaAndCheckpointFiles$1(SnapshotManagement.scala:98)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withOperationTypeTag(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:139)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordOperation(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordDeltaOperation(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.listDeltaAndCheckpointFiles(SnapshotManagement.scala:94)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$getLogSegmentForVersion$1(SnapshotManagement.scala:140)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getLogSegmentForVersion(SnapshotManagement.scala:137)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.getLogSegmentForVersion$(SnapshotManagement.scala:134)
	at com.databricks.sql.transaction.tahoe.DeltaLog.getLogSegmentForVersion(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$updateInternal$1(SnapshotManagement.scala:482)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withOperationTypeTag(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:139)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordOperation(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordDeltaOperation(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.updateInternal(SnapshotManagement.scala:481)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.updateInternal$(SnapshotManagement.scala:480)
	at com.databricks.sql.transaction.tahoe.DeltaLog.updateInternal(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$update$2(SnapshotManagement.scala:438)
	at com.databricks.sql.transaction.tahoe.DeltaLog.lockInterruptibly(DeltaLog.scala:197)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.$anonfun$update$1(SnapshotManagement.scala:438)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordFrameProfile(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.update(SnapshotManagement.scala:437)
	at com.databricks.sql.transaction.tahoe.SnapshotManagement.update$(SnapshotManagement.scala:433)
	at com.databricks.sql.transaction.tahoe.DeltaLog.update(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.$anonfun$startTransactionForTable$1(DeltaLogEdge.scala:71)
	at com.databricks.sql.transaction.tahoe.NoOpTransactionExecutionObserver$.startingTransaction(TransactionExecutionObserver.scala:98)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.startTransactionForTable(DeltaLogEdge.scala:70)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.startTransactionForTable$(DeltaLogEdge.scala:69)
	at com.databricks.sql.transaction.tahoe.DeltaLog.startTransactionForTable(DeltaLog.scala:75)
	at com.databricks.sql.transaction.tahoe.DeltaLog.startTransaction(DeltaLog.scala:217)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:88)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1585)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:87)
	at com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:165)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:590)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:168)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:590)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:566)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:427)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)
	at com.volvocars.rvdc.common.DeltaTableHelpers$.saveOrUpdate(DeltaTableHelpers.scala:61)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.decrytpAndMerge(DecryptMergeMemreadBase.scala:318)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.$anonfun$run$1(DecryptMergeMemreadBase.scala:80)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.$anonfun$run$1$adapted(DecryptMergeMemreadBase.scala:65)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.run(DecryptMergeMemreadBase.scala:65)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.processMemRead(DecryptMergeMemreadBase.scala:59)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadBase.run(DecryptMergeMemreadBase.scala:38)
	at com.volvocars.rvdc.datajobs.DecryptMergeMemreadSpin$.main(DecryptMergeMemreadSpin.scala:7)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:43)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw$$iw.<init>(command--1:45)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw$$iw.<init>(command--1:47)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw$$iw.<init>(command--1:49)
	at $line3860880a722c453fae7be822f691a0b525.$read$$iw.<init>(command--1:51)
	at $line3860880a722c453fae7be822f691a0b525.$read.<init>(command--1:53)
	at $line3860880a722c453fae7be822f691a0b525.$read$.<init>(command--1:57)
	at $line3860880a722c453fae7be822f691a0b525.$read$.<clinit>(command--1)
	at $line3860880a722c453fae7be822f691a0b525.$eval$.$print$lzycompute(<notebook>:7)
	at $line3860880a722c453fae7be822f691a0b525.$eval$.$print(<notebook>:6)
	at $line3860880a722c453fae7be822f691a0b525.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1003)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:956)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:634)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:611)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)
	at java.lang.Thread.run(Thread.java:750)
23/02/08 00:30:29 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 3727427424790412939, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 3727427424790412939. Created 3727427424790412939 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
